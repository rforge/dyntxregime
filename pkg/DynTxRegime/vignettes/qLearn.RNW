\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{Sweave}
\newcommand{\bma}[1]{\mbox{\boldmath $#1$}}
\newcommand{\T}{\intercal}
\newcommand{\bX}{ {\bma{X}} }
\newcommand{\bx}{ {\bma{x}} }
\newcommand{\bH}{ {\bma{H}} }
\newcommand{\bh}{ {\bma{h}} }
\newcommand{\proglang}[1]{\texttt{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\texttt{#1}}

  
\begin{document}
\title{$Q$-learning in \tt{R} using \tt{DynTxRegime}: } 
\author{Kristin A. Linn, Eric B. Laber, Leonard A. Stefanski, Shannon T. Holloway} 
\maketitle

\begin{abstract}
\vspace{-0.08in}
  Chronic illness treatment strategies must adapt to the 
  evolving health status of the patient receiving treatment.  
  Data-driven dynamic treatment regimes can offer clinicians 
  and intervention scientists guidance on how to treat patients over time 
  to bring about the most favorable clinical outcome on average.  
  $Q$-learning is a method for estimating optimal dynamic treatment regimes. 
  The \proglang{R} package \pkg{DynTxRegime} provides functions for
  implementing the $Q$-learning algorithm. We demonstrate how to
  estimate a two-stage optimal treatment policy with $Q$-learning
  using a generated data set, \code{bmiData}, that mimics a two-stage
  randomized body mass index reduction trial with binary treatments at
  each stage.
\end{abstract}
\hrule
\vspace{0.1in}

\noindent {\em Keywords:} Q-learning; 
Dynamic Treatment Regimes; Dynamic Programming; SMART design.

%\VignetteIndexEntry{Using iqLearn}

\section{Introduction}

In practice, clinicians and intervention scientists must adapt
treatment recommendations in response to the uniquely evolving health
status of each patient.  Dynamic treatment regimes (DTRs) formalize
this treatment process as a sequence of decision rules, one for each
treatment decision, that map current and past patient information to
a recommended treatment.  A DTR is said to be optimal for a
pre-specified desirable outcome if 
it yields the maximal expected outcome when applied to assign treatment
to a population of interest. 

With the potential for better patient outcomes, reduced treatment
burden, and lower cost, there is growing interest in personalized treatment
strategies \citep{Ham+Col:10, pmc:10}.   
Sequential, Multiple Assignment, Randomized Trials
\citep[SMARTs][]{Lav+Daw:04, Mur:05a} are designed to estimate
optimal DTRs.  In a SMART, subjects are randomized to treatment at
each decision
point or \emph{stage} of the trial. Figure~\ref{smart} contains a
visual representation of an example SMART design, where all subjects
receive the same treatment at baseline (e.g., a standard of
care) and are then randomized (represented by gold circles) at the
start of the first stage to one of two treatment categories:
``switch'' or ``augment'' current treatment.  At the start of the
second stage, subjects are again randomized to either switch or
augment their current treatment(s). There are many variations of this
design; for example, more than two treatments can be offered at each
stage, and for ethical reasons it is common to include an option for
baseline or first-stage 
responders to continue their currently successful
treatment. Although it is possible to design a trial with additional
stages, two stage SMARTs are common, as evidenced by many recently
completed and ongoing SMARTs. For a list of SMARTs that have
finished or are in the field, see \citet{psu:12} and Eric Laber's
current list \citet{eblSmart}. With additional
randomizations beyond one or two stages, the number of patients
assigned to each sequence of 
treatments decreases, along with the power to estimate optimal
decisions in the later stages. The sequential randomization scheme in
SMARTs guarantees that there are no confounders that influence which
types of subjects follow each of the possible treatment sequences. To
keep our discussion focused, we will work under the assumption of a
two-stage SMART with randomized binary treatments at each
stage. However, all of the methods discussed here apply to observational
data when additional assumptions are made on the treatment assignment
mechanism \citep[see, for example,][]{Mur:03}.
\begin{figure}
\begin{center}
\includegraphics[scale=.6]{figure1.jpeg}
\end{center}
\caption{Example of a SMART design with two randomized stages and two
  treatment options at each stage. Randomizations are represented by
  gold circles.}\label{smart}
\end{figure}



We introduce the $Q$-learning methods 
implemented in package \pkg{DynTxRegime} in \proglang{R}
 \citep{R} for estimating 
optimal DTRs from data obtained from a two stage
trial with two treatments at each stage. 
Section 2 introduces $Q$-learning, and a case-study is
provided in Section 3 illustrating the $Q$-learning
capabilities of the \pkg{DynTxRegime} package. 

\section{$Q$-learning}
We assume data are collected from a two-stage randomized trial with 
binary treatments at each stage, resulting in $n$ $\hbox{i.i.d.}$ patient
trajectories of the form $\allowbreak (\bX_{1}, A_{1}, \bX_{2}, A_{2},
Y)$. Note that binary treatments are chosen here for convenience; the
$Q$-learning methods implemented in \pkg{DynTxRegime} do not
require binary treatment options. 
The variables in the trajectory are: baseline covariates,
$\bX_{1} \in \mathbb{R}^{p_1}$; first-stage randomized treatment,
$A_{1} \in \{-1, 1\}$; covariates collected during the first-stage but
prior to second-stage treatment assignment, $X_{2} \in
\mathbb{R}^{p_2}$; second-stage randomized treatment, $A_{2}\in
\lbrace -1, 1\rbrace$; and the response, $Y \in \mathbb{R}$, collected 
at the conclusion of the trial. We assume $Y$ has been coded so that
higher values indicate more positive clinical outcomes -- this
is required for all methods in the \pkg{DynTxRegime} package.  
To simplify
notation, we group variables collected prior to each treatment
randomization into a history vector $\bH_{t}$, $t=1,2$. That is,
$\bH_{1} = \bX_{1}$ and $\bH_{2}=(\bX_{1}^{\top}, A_{1},
\bX_{2}^{\top})^{\top}$. 

A DTR is a pair of functions ${\boldsymbol \pi} = (\pi_1, \pi_2)$ where
$\pi_t$ maps the domain of $\bH_t$ into the space of 
available treatments $\lbrace -1, 1\rbrace$.  
Under ${\boldsymbol \pi}$ a patient presenting at time $t$ with 
history $\bH_t=\bh_t$ is assigned treatment $\pi_t(\bh_t)$.  
The goal is to estimate a DTR, which 
when applied in a population of patients of interest, the
expected outcome is maximized. Define the
value of a fixed regime ${\boldsymbol \pi}$ as $V^{{\boldsymbol \pi}} \triangleq
\mathbb{E}^{{\boldsymbol \pi}}(Y)$, where $\mathbb{E}^{{\boldsymbol \pi}}$ denotes the
expectation when treatment is assigned according to the policy ${\boldsymbol \pi}$.  
The optimal treatment regime, ${\boldsymbol \pi}^{\hbox{\scriptsize  opt}}$,
maximizes the value function:
\begin{equation*}
  \mathbb{E}^{{\boldsymbol \pi}^{\hbox{\scriptsize  opt}}}(Y) = 
\sup_{{\boldsymbol \pi}} \mathbb{E}^{{\boldsymbol \pi}} Y.
\end{equation*}
In the next section, we explain how an optimal regime can be 
estimated from data using $Q$-learning as implemented in the 
\pkg{DynTxRegime} package.  

\subsection{$Q$-learning}

$Q$-learning \citep{Wat:89, Wat+Day:92, Mur:05b} is an approximate 
dynamic programming algorithm that can be used to estimate an optimal 
DTR from observational or randomized study data.  
For convenience, we limit our discussion to two-stage $Q$-learning;
the methods can be easily extended to more stages. Define the
$Q$-functions:
\begin{eqnarray*}
  Q_{2}(\bh_{2}, a_{2}) &\triangleq& \mathbb{E}(Y | \bH_{2}=\bh_{2},
  A_{2}=a_{2}), \\
  Q_{1}(\bh_{1}, a_{1}) &\triangleq& \mathbb{E} \left( \max_{a_{2}
      \in \{-1, 1\}} Q_{2}(\bH_{2}, a_{2}) | \bH_{1}=\bh_{1},
    A_{1}=a_{1} \right). 
\end{eqnarray*}
The $Q$-function at stage two measures the {\bf Q}uality of assigning
$a_{2}$ to a patient presenting with history $\bh_{2}$. Similarly,
$Q_{1}$ measures the quality of assigning $a_{1}$ to a patient with
$\bh_{1}$, assuming an optimal decision rule will be followed at stage
two. Were the $Q$-functions known, dynamic programming \citep{Bel:57}
gives the optimal solution, 
$\pi_{t}^{\hbox{ \scriptsize opt}}(\bh_{t}) = \arg \max_{a_{t}
  \in \{ -1, 1\}} Q_{t}(\bh_{1}, a_{t})$.  Since the underlying
distribution of the patient histories is not known, the conditional
expectations that define the $Q$-functions are unknown and must be
approximated.  $Q$-learning approximates the $Q$-functions with
regression models; commonly linear models are chosen in practice
because they yield simple, interpretable models.  We will consider
linear models of the form:
% \begin{equation*}
$Q_{t}(\bh_{t}, a_{t}; \beta_{t}) = \bh_{t0}^{\top}\beta_{t0} +
a_{t}\bh_{t1}^{\top}\beta_{t1}, \;  t=1,2,$
% \end{equation*}
where $\bh_{t0}$ and $\bh_{t1}$ include a subset of variables
collected in $\bh_{t}$. Define $\beta_{t} \triangleq (\beta_{t0}^{\top},
\beta_{t1}^{\top})^{\top}$. The $Q$-learning algorithm for a two-stage
trial is given below.

For convenience, define 
$m(\bH_{t}; \beta_{t}) \triangleq \bH_{t0}^{\top}\beta_{t0}$, and 
$\Delta (\bH_{t}; \beta_{t}) \triangleq \bH_{t1}^{\top}\beta_{t1}.$
We call the first term the \emph{main effect function} at stage $t$ and the
second the \emph{contrast function} at stage $t$.


\vspace{5mm}

\noindent {\bf Two-stage $Q$-learning Algorithm:}    
\begin{center}
  \begin{tabular}{| l p{3.3in} |}
    \hline
    Q1. Modeling: & Regress $Y$ on $\bH_{20}, \bH_{21}, A_{2}$ to
    obtain \\ 
    & $\widehat{Q}_{2} (\bH_{2}, A_{2}; \widehat{\beta}_{2}) =
    \bH_{20}^{\top}\widehat{\beta}_{20} +
    A_{2}\bH_{21}^{\top}\widehat{\beta}_{21}$. \\ 
    & \\
    Q2. Maximization: & Define $\widetilde{Y} \triangleq \max_{a_{2} \in \{
      -1,1\}} \widehat{Q}_{2}(\bH_{2}, a_{2},
    \widehat{\beta}_{2})$. \\  
    & $\widetilde{Y} = \bH_{20}^{\top}\widehat{\beta}_{20} +
    |\bH_{21}^{\top}\widehat{\beta}_{21}|$ is the predicted future
    outcome assuming  the optimal decision is made at stage two. \\  
    & \\
    Q3. Modeling: & Regress $\widetilde{Y}$ on $\bH_{10}, \bH_{11}, A_{1}$ to obtain \\
    & $\widehat{Q}_{1} (\bH_{1}, A_{1}; \widehat{\beta}_{1}) =
    \bH_{10}^{\top}\widehat{\beta}_{10} +
    A_{1}\bH_{11}^{\top}\widehat{\beta}_{11}$. \\ 
    \hline
  \end{tabular}
\end{center}
The $t^{\hbox{\scriptsize th}}$-stage optimal decision rule then
assigns the treatment $a_{t}$ that maximizes the estimated
$Q_{t}$-function, 
\begin{equation*}
  \widehat{\pi}_{t}(\bh_{t}) = \arg\max_{a_{t}}
  \widehat{Q}_{t}(\bh_{t}, a_{t}; \widehat{\beta}_{t}).
\end{equation*}
and the value function is defined as
\begin{equation*}
  V_{t}(\bh_{t}) = \max_{a_{t} \in \{ -1, 1\}}
  \widehat{Q}_{t}(\bh_{t}, a_{t}; \widehat{\beta}_{t}).
\end{equation*}

The first modeling step in the $Q$-learning algorithm is a standard
multiple regression problem to which common model building and model
checking techniques can be applied to find a parsimonious, well-fitting
model.   The second modeling
step (Q3) requires modeling the conditional expectation of
$\widetilde{Y}$. This can be written as
\begin{eqnarray}
  \label{q1fn}
  Q_{1}(\bH_{1}, A_{1}) &=& \mathbb{E}(\widetilde{Y} | \bH_{1}, A_{1})
  \nonumber \\ 
  &=& \mathbb{E} (V_{t}(\bh_{t})  \; |\;  \bH_{1}, A_{1}).
\end{eqnarray}

  \section[Using the DynTxRegime Package]{Using the \pkg{DynTxRegime} Package} 
  
  \subsection{Preparing dataset bmiData}
  
The examples in this section will be illustrated using a
simulated dataset called \pkg{bmiData} which is included in the
\pkg{DynTxRegime} package. The data are generated to mimic a two-stage
SMART of body mass index (BMI) reduction with two treatments at each
stage.  The variables, treatments, and outcomes in \pkg{bmiData} were
based on a small subset of variables collected in a clinical trial
studying the effect of meal replacements (MRs) on weight loss and BMI
reduction in obese adolescents; see \citet{Ber+etal:10} for a complete
description of the original randomized trial. Descriptions of
the generated variables in \pkg{bmiData} are given in Table
(\ref{vars}). Baseline covariates include \code{gender}, \code{race},
\code{parent\_BMI}, and \code{baseline\_BMI}. Four- and twelve-month
patient BMI measurements were also included to reflect the original
trial design. In the generated data,
treatment was randomized to meal replacement (MR) or conventional diet
(CD) at both stages, each with probability 0.5. In the original
study, patients randomized to CD in stage one remained on CD with
probability one in stage two.  Thus, our generated data arises from a
slightly difference
design than that of the original trial.  In addition,
some patients in the original data set were missing the final twelve
month response as well as various first- and second-stage covariates.
Our generated data is complete, and the illustration of 
$Q$-learning with \pkg{DynTxRegime} that follows is presented under the
assumption that missing data have been addressed prior
to using these methods (for example, using an appropriate imputation strategy). 
  \begin{table}
    \begin{center}
      \begin{tabular}{|lcp{11cm}|}\hline
        \code{gender} $\in \{0, 1\}$ & \,:\,& patient gender, coded
        female (0) and male (1). \\   
        \code{race} $\in \{0, 1\}$ & \, : \, & patient race, coded
        African American (0) or other (1). \\ 
        \code{parent\_BMI} $\in \mathbb{R}$ & \, : \, & parent BMI
        measured at baseline. \\ 
        \code{baseline\_BMI} $\in \mathbb{R}$ & \, : \, & patient BMI
        measured at baseline. \\ 
        \code{A1} $\in \lbrace -1, 1\rbrace$ &\,:\,& first-stage randomized
        treatment, coded so that \code{A1} = 1 corresponds to meal replacement
        (MR) and \code{A1} = -1 corresponds to conventional diet
        (CD). \\  
        \code{month4\_BMI} $\in \mathbb{R}$ & \, : \, & patient BMI
        measured at month 4. \\ 
        \code{A2} $\in \lbrace -1, 1\rbrace$ &\,:\,& second-stage randomized
        treatment, coded so that \code{A2} = 1 corresponds to meal replacement
        (MR) and \code{A2} = -1 corresponds to conventional diet
        (CD). \\  
        \code{month12\_BMI} $\in \mathbb{R}$ &\,:\,& patient BMI
        measured at month 12.\\ 
        \hline
      \end{tabular}
      \caption{Description of variables in \pkg{bmiData}.}\label{vars}
    \end{center}
  \end{table}

<<echo=false>>=
options(width=70)
@ 
After installing {\bf DynTxRegime}, load the package:
<<>>=
library (DynTxRegime)
@ 
Next, load {\bf bmiData} into the workspace with
<<>>=
data (bmiData)
@ 
The generated dataset {\bf bmiData} is a data frame with 210 rows
corresponding to patients and 8 columns corresponding to covariates,
BMI measurements, and assigned treatments.   
<<>>=
dim (bmiData)
head (bmiData)
@ 
Recode treatments Meal Replacement (MR) and Conventional Diet (CD) as
1 and -1, respectively.
<<>>=
bmiData$A1[which (bmiData$A1=="MR")] = 1
bmiData$A1[which (bmiData$A1=="CD")] = -1
bmiData$A2[which (bmiData$A2=="MR")] = 1
bmiData$A2[which (bmiData$A2=="CD")] = -1
bmiData$A1 = as.numeric (bmiData$A1)
bmiData$A2 = as.numeric (bmiData$A2)
@ 
We use the negative percent change in BMI at month 12 from baseline as
our final outcome:
<<>>=
y <- -100*(bmiData$month12_BMI -
           bmiData$baseline_BMI)/bmiData$baseline_BMI
@ 
Thus, higher values indicate greater BMI loss, a desirable clinical
outcome. We will next show how to implement $Q$-learning with the 
\pkg{DynTxRegime} package to obtain an estimate of the optimal DTR,
$\widehat{\boldsymbol \pi} = 
(\widehat{\pi}_{1},
\widehat{\pi}_{2})$, that
maximizes the expected BMI reduction. 

\subsection{Q-learning functions}

\noindent {\bf Defining models}
The current version of the \pkg{DynTxRegime} package uses the \pkg{modelObj}
package to define models and regression methods to be used for analysis. This
choice is intended to generalize the methods available in the 
\pkg{DynTxRegime} package beyond simple linear models. 
The reader is referred to the vignette for the \pkg{modelObj}
package for details. In general, the model, regression
method, and prediction method are specified as follows:
<<>>=
moMain <- buildModelObj(model = ~ gender + parent_BMI + month4_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))
@ 

where \code{solver.method} can be any
available \proglang{R} regression method  and \code{predict.method} is
its accompanying prediction method. Note that a prediction method {\bf must}
be available for the specified regression method.
 
\subsection{$Q$-learning algorithm}

Function 
\code{qLearn()} implements a single regression step of the $Q$-learning
algorithm. For all stages, two objects of class \code{modelObj}
are used to communicate the models. The main effect model,
the \proglang{R} function to be used to obtain parameter estimates, and 
the \proglang{R} function to be used to obtain predictions are defined by
\code{moMain}. Similarly, \code{moCont} defines the contrast model.
If the \proglang{R} methods and method arguments specified in \code{moMain} and
\code{moCont} are the same (except of course for the model), the user
can request that the models be combined into a single formula and fit in a single
step by specifying \code{iter}=0. If different \proglang{R} methods are 
required, the two models will be fit iteratively, and 
\code{iter} must be $>= 1$, indicating the maximum number of iterations 
allowed to attain convergence. 
\code{data} is a data.frame of covariates and
treatments
And, \code{txName} is a character string indicating the
column header of data containing the treatment variable for the
stage under analysis.


For the final-stage regression (second-stage in our example),
the final outcome variable is passed to \code{qLearn()} through input 
\code{response}.
<<>>=
moMain <- buildModelObj(model = ~ gender + parent_BMI + month4_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

moCont <- buildModelObj(model = ~ parent_BMI + month4_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

fitQ2 <- qLearn(moMain = moMain, 
                moCont = moCont,
                data = bmiData, 
                txName = 'A2', 
                response = y,
                iter = 0)
fitQ2
@ 
Method \code{qLearn()} returns an object of class \code{qLearn}.
There are several methods available for objects of class \code{qLearn},
including: \code{coef()}, \code{contrast()}, \code{getFit()}, \code{main()},
 \code{optTx()}, \code{plot()}, \code{residuals()}, \code{summary()}, and
\code{ValueFuncs()}. 
Methods \code{coef()} and \code{plot()} 
are applied to the fit objects returned by the specified \proglang{R}
regression method, and thus must be defined by the regression 
method(s) specified in \code{moMain} and/or \code{moCont}. 
Method \code{getFit()} retrieves the value object returned by the regression
method(s), and thus all functionality available through said method can be 
accessed. If an iterative fit is specified, a list containing the 
main effect value object \code{\$mainEffects} 
and the contrast value object \code{\$contrast} is returned.
Methods \code{contrast()} and \code{main()} return the fitted contrast 
function and main effect function, respectively.
Method \code{optTx()} returns the recommended second-stage treatment for
all patients in the study, or if data for a new patient is provided,
the recommended treatment.
Finally, method \code{ValueFuncs()} returns the fitted $Q$-function at each
treatment option.

For all other stages of the analysis, the previous-stage \code{qLearn} 
object is passed through input \code{object}; \code{response} is provided
only for the final stage analysis. In our example, the 
first-stage analysis follows:
<<>>=
moMain <- buildModelObj(model = ~ gender + race + parent_BMI + baseline_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

moCont <- buildModelObj(model = ~ gender + parent_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

fitQ1 <- qLearn(moMain = moMain, 
                moCont = moCont,
                data = bmiData, 
                txName = 'A1', 
                object = fitQ2,
                iter = 0)
fitQ1
@ 
Again, an object of class \code{qLearn} is returned.
See above for available methods for objects of this class. 

\vspace{5mm}

\noindent {\bf Recommend treatment with optTx()}

\vspace{2mm}

To recommend the $Q$-learning estimated optimal treatments for a new 
patient based on observed histories, method \code{optTx()} 
is available. It requires the observed history vectors for the new
patient. For the first-stage optimal treatment:
<<>>=
newdata = data.frame(1, 1, 30, 35)
colnames(newdata) <- c("gender","race","parent_BMI","baseline_BMI")
optQ1 = optTx(x=fitQ1, newdata=newdata);
optQ1
@ 
As displayed above, a list is returned by \code{optTx()} that includes
\code{\$valueFuncs}, the vector of the first-stage $Q$-functions at each
treatment value, as well as the recommended first-stage
treatment for that patient, \code{\$optTx}. 

The recommended second-stage optimal treatment is similarly obtained:
<<>>= 
newdata = data.frame(1, 30, 45)
colnames(newdata) <- c("gender","parent_BMI","month4_BMI")
optQ2 = optTx(x = fitQ2, 
              newdata = newdata);
optQ2
@ 

\vspace{5mm}

\noindent {\bf Feasible Treatment Sets}

In some trial settings, the treatment options available to a patient at each
decision point depends on the patient history and/or prior treatments.
For example, in the original study upon which our bmiData is based, patients 
randomized to CD in stage one remained on CD with probability one 
in stage two. Thus, for patients randomized to CD in stage one,
the set of feasible treatment options at the second-stage
is restricted to $\{CD\}$ or $\{-1\}$. For all other patients,
the set of feasible treatment options at the second-stage is $\{MR,CD\}$
or $\{-1,1\}$.
In such circumstances, patients that have only one treatment option available
should not be included in the analysis. 

The method \code{qLearn()} has an optional input variable, \code{fSet},
which allows the user to specify the
conditions under which treatments options are restricted to a subset. 
These rules are defined by the user as a function. 
The function takes as input \code{data}, the covariates and treatments
of a single patient, and returns
a vector of treatment options available to that patient. For the second stage
of the original study, this rule would take the following form
<<>>= 
fSet <- function(data){
          if(data$A1 == -1){
            return(-1)
          } else {
            return(c(-1,1))
          }
        }
@ 
The second-stage $Q$-learning analysis would be
<<>>=
moMain <- buildModelObj(model = ~ gender + parent_BMI + month4_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

moCont <- buildModelObj(model = ~ parent_BMI + month4_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

fitQ2 <- qLearn(moMain = moMain, 
                moCont = moCont,
                data = bmiData, 
                txName = 'A2', 
                response = y,
                fSet = fSet,
                iter = 0)
fitQ2
@ 

Note, a \code{qLearn} object is again returned, to which the previously
discussed methods can be applied.
However, the vectors or matrices returned by $\code{contrast()}$, $\code{main()}$,
and $\code{ValueFuncs()}$ will include $NA$s for the patients not included in the
regression (i.e., have only one treatment option available).

For the first-stage analysis, all treatments are available to all patients,
and the input variable \code{fSet} is not required. Here, the
analysis is initiated as was previously shown. The $\code{qLearn}$ object
returned by the stage two analysis is provided as input and
includes all necessary information regarding previous stage treatment rules.
<<>>=
moMain <- buildModelObj(model = ~ gender + race + parent_BMI + baseline_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

moCont <- buildModelObj(model = ~ gender + parent_BMI,
                   solver.method = 'lm',
                   predict.method = 'predict.lm',
                   predict.args = list(type='response'))

fitQ1 <- qLearn(moMain = moMain, 
                moCont = moCont,
                data = bmiData, 
                txName = 'A1', 
                object = fitQ2,
                iter = 0)
fitQ1
@ 

When treatment recommendations are obtained using \code{optTx},
the feasible set of treatment options is included when
determining the optimal treatment. 
<<>>=
newdata = data.frame(1, 1, 32, 35)
colnames(newdata) <- c("gender","race","parent_BMI","baseline_BMI")
optQ1 = optTx(x=fitQ1, newdata=newdata);
optQ1
@ 
<<>>=
newdata = data.frame(1, 30, 45,optQ1$optTx)
colnames(newdata) <- c("gender","parent_BMI","month4_BMI","A1")
optQ2 = optTx(x = fitQ2, 
              newdata = newdata);
optQ2
@ 
Note that only one value function is returned by \code{optTx()} 
for the second-stage treatment recommendation in this
example because the first-stage treatment was $-1$.
\vspace{2mm}
<<>>=
newdata = data.frame(1, 1, 28, 35)
colnames(newdata) <- c("gender","race","parent_BMI","baseline_BMI")
optQ1 = optTx(x=fitQ1, newdata=newdata);
optQ1
@ 
<<>>=
newdata = data.frame(1, 30, 45,optQ1$optTx)
colnames(newdata) <- c("gender","parent_BMI","month4_BMI","A1")
optQ2 = optTx(x = fitQ2, 
              newdata = newdata);
optQ2
@ 
\vspace{2mm}
In addition to specifying feasible treatment sets for each patient, the 
\code{qLearn()} method allows for unique models to be specified for
each subset of treatments. For further information regarding this
feature, please see the documentation for \code{subsetModel}.

\section*{Acknowledgments}

The authors would like to thank Dr.\ Rene\'{e} Moore for discussions
about meal replacement therapy for obese adolescents that informed the
data generation model.

\bibliographystyle{apalike}
\bibliography{iq_cites.bib}

\end{document}
